{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mon89/Spam-Classification/blob/main/spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82853bb4-02f1-4762-962b-a287da86c1c9",
      "metadata": {
        "id": "82853bb4-02f1-4762-962b-a287da86c1c9"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a682f7be-9c81-4634-9cd1-b539f42d8189",
      "metadata": {
        "id": "a682f7be-9c81-4634-9cd1-b539f42d8189"
      },
      "outputs": [],
      "source": [
        "# URL of the dataset\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf60eec0-949f-4c89-872a-274c0cc34208",
      "metadata": {
        "id": "bf60eec0-949f-4c89-872a-274c0cc34208",
        "outputId": "9c361ceb-9453-4c55-f596-f908a600c47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download successful\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Download successful\")\n",
        "else:\n",
        "    print(\"Failed to download the dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfabce63-1a94-43c9-900a-f2d1ba32b95f",
      "metadata": {
        "id": "dfabce63-1a94-43c9-900a-f2d1ba32b95f",
        "outputId": "2b44b871-9bac-442c-8943-7cb6072640b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extraction successful\n"
          ]
        }
      ],
      "source": [
        "# Extract the dataset\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "    z.extractall(\"sms_spam_collection\")\n",
        "    print(\"Extraction successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3751f108-6e84-4042-9e0d-8bfb43ffd0fa",
      "metadata": {
        "id": "3751f108-6e84-4042-9e0d-8bfb43ffd0fa",
        "outputId": "962a3370-8228-4130-9d82-f30ee4d20aef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted files: ['readme', 'SMSSpamCollection']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = os.listdir(\"sms_spam_collection\")\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d79143-bcdc-487b-a8d3-d82c99374df4",
      "metadata": {
        "id": "c6d79143-bcdc-487b-a8d3-d82c99374df4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\n",
        "    \"sms_spam_collection/SMSSpamCollection\",\n",
        "    sep=\"\\t\",\n",
        "    header=None,\n",
        "    names=[\"label\", \"message\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d123402-4f3c-497d-84a7-daeae9552fff",
      "metadata": {
        "id": "6d123402-4f3c-497d-84a7-daeae9552fff",
        "outputId": "dbc9ab78-9bb4-4cf6-b833-b6cbb534ab2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------- HEAD --------------------\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "-------------------- DESCRIBE --------------------\n",
            "       label                 message\n",
            "count   5572                    5572\n",
            "unique     2                    5169\n",
            "top      ham  Sorry, I'll call later\n",
            "freq    4825                      30\n",
            "-------------------- INFO --------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 2 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   label    5572 non-null   object\n",
            " 1   message  5572 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 87.2+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Display basic information about the dataset\n",
        "print(\"-------------------- HEAD --------------------\")\n",
        "print(df.head())\n",
        "print(\"-------------------- DESCRIBE --------------------\")\n",
        "print(df.describe())\n",
        "print(\"-------------------- INFO --------------------\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08b34b2-0062-4786-8724-2f7f96c7a5d6",
      "metadata": {
        "id": "d08b34b2-0062-4786-8724-2f7f96c7a5d6",
        "outputId": "f8a175e5-784d-41ca-9549-db5597c347a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values:\n",
            " label      0\n",
            "message    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d7be0c8-047c-44ad-a9cd-e38a816f9e55",
      "metadata": {
        "id": "7d7be0c8-047c-44ad-a9cd-e38a816f9e55",
        "outputId": "64da4f5b-a5e0-412a-bcc3-ffd54fabf56f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duplicate entries: 403\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicates\n",
        "print(\"Duplicate entries:\", df.duplicated().sum())\n",
        "\n",
        "# Remove duplicates if any\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b093976d-8eee-4baa-b318-7d6fe4ef4ae5",
      "metadata": {
        "id": "b093976d-8eee-4baa-b318-7d6fe4ef4ae5",
        "outputId": "f9bdfd15-25aa-4dd7-f262-cfd3e8729ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BEFORE ANY PREPROCESSING ===\n",
            "  label                                            message\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/khin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /Users/khin/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /Users/khin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Download the necessary NLTK data files\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "print(\"=== BEFORE ANY PREPROCESSING ===\")\n",
        "print(df.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a692a3fe-31fd-492b-a13b-12692a5f4d3d",
      "metadata": {
        "id": "a692a3fe-31fd-492b-a13b-12692a5f4d3d",
        "outputId": "caf76a42-18ef-45f2-bd78-79a058e2cdc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/khin/AI_infosec_HTB/Spam Detection\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "012980a4-c5d5-44c0-9f45-ca3ffc72ef1f",
      "metadata": {
        "id": "012980a4-c5d5-44c0-9f45-ca3ffc72ef1f",
        "outputId": "019f9d1c-5e58-4315-a32a-df8fe5200c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER LOWERCASING ===\n",
            "0    go until jurong point, crazy.. available only ...\n",
            "1                        ok lar... joking wif u oni...\n",
            "2    free entry in 2 a wkly comp to win fa cup fina...\n",
            "3    u dun say so early hor... u c already then say...\n",
            "4    nah i don't think he goes to usf, he lives aro...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Convert all message text to lowercase\n",
        "df[\"message\"] = df[\"message\"].str.lower()\n",
        "print(\"\\n=== AFTER LOWERCASING ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca9a1d6-4bf4-4ea1-9655-6187e8093b92",
      "metadata": {
        "id": "fca9a1d6-4bf4-4ea1-9655-6187e8093b92",
        "outputId": "ee30dd11-597e-43aa-e3fe-90e88d6431e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
            "0    go until jurong point crazy available only in ...\n",
            "1                              ok lar joking wif u oni\n",
            "2    free entry in  a wkly comp to win fa cup final...\n",
            "3          u dun say so early hor u c already then say\n",
            "4    nah i dont think he goes to usf he lives aroun...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
        "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01b14e6-9e54-4691-b453-8e14d0c6cb35",
      "metadata": {
        "id": "c01b14e6-9e54-4691-b453-8e14d0c6cb35",
        "outputId": "1ad4f464-d02b-4170-bc55-d0efe45e4d64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER TOKENIZATION ===\n",
            "0    [go, until, jurong, point, crazy, available, o...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, in, a, wkly, comp, to, win, fa, ...\n",
            "3    [u, dun, say, so, early, hor, u, c, already, t...\n",
            "4    [nah, i, dont, think, he, goes, to, usf, he, l...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split each message into individual tokens\n",
        "df[\"message\"] = df[\"message\"].apply(word_tokenize)\n",
        "print(\"\\n=== AFTER TOKENIZATION ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ce52ad-09a0-4a17-82e3-1d53168df50a",
      "metadata": {
        "id": "f4ce52ad-09a0-4a17-82e3-1d53168df50a",
        "outputId": "17593671-2b56-4b52-ff3a-74d8587dafd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER REMOVING STOP WORDS ===\n",
            "0    [go, jurong, point, crazy, available, bugis, n...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, wkly, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, early, hor, u, c, already, say]\n",
            "4    [nah, dont, think, goes, usf, lives, around, t...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a set of English stop words and remove them from the tokens\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "print(\"\\n=== AFTER REMOVING STOP WORDS ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8fefefc-969f-48dc-beb0-e73df389ade8",
      "metadata": {
        "id": "f8fefefc-969f-48dc-beb0-e73df389ade8",
        "outputId": "94e1e30f-75fd-4f07-fbad-0085b5332812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER STEMMING ===\n",
            "0    [go, jurong, point, crazi, avail, bugi, n, gre...\n",
            "1                         [ok, lar, joke, wif, u, oni]\n",
            "2    [free, entri, wkli, comp, win, fa, cup, final,...\n",
            "3        [u, dun, say, earli, hor, u, c, alreadi, say]\n",
            "4    [nah, dont, think, goe, usf, live, around, tho...\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Stem each token to reduce words to their base form\n",
        "stemmer = PorterStemmer()\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "print(\"\\n=== AFTER STEMMING ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf08873-7a9b-47cb-9666-92d9ab87b067",
      "metadata": {
        "id": "bcf08873-7a9b-47cb-9666-92d9ab87b067",
        "outputId": "824d6fca-6204-4a2b-c182-be201e69634d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== AFTER JOINING TOKENS BACK INTO STRINGS ===\n",
            "0    go jurong point crazi avail bugi n great world...\n",
            "1                                ok lar joke wif u oni\n",
            "2    free entri wkli comp win fa cup final tkt st m...\n",
            "3                  u dun say earli hor u c alreadi say\n",
            "4            nah dont think goe usf live around though\n",
            "Name: message, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Rejoin tokens into a single string for feature extraction\n",
        "df[\"message\"] = df[\"message\"].apply(lambda x: \" \".join(x))\n",
        "print(\"\\n=== AFTER JOINING TOKENS BACK INTO STRINGS ===\")\n",
        "print(df[\"message\"].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ffbe1d6-fe74-42fe-a562-0d9fd20e8d0f",
      "metadata": {
        "id": "5ffbe1d6-fe74-42fe-a562-0d9fd20e8d0f"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
        "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the message column\n",
        "X = vectorizer.fit_transform(df[\"message\"])\n",
        "\n",
        "# Labels (target variable)\n",
        "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9604ef07-b54c-4b83-b9c4-d3c3ad77d806",
      "metadata": {
        "id": "9604ef07-b54c-4b83-b9c4-d3c3ad77d806"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Build the pipeline by combining vectorization and classification\n",
        "pipeline = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", MultinomialNB())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5792112-5216-4d8f-84b9-421a6d02984a",
      "metadata": {
        "id": "a5792112-5216-4d8f-84b9-421a6d02984a",
        "outputId": "76b953a8-de18-4d4b-a76b-ff46b2dcce4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model parameters: {'classifier__alpha': 0.25}\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
        "}\n",
        "\n",
        "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")\n",
        "\n",
        "# Fit the grid search on the full dataset\n",
        "grid_search.fit(df[\"message\"], y)\n",
        "\n",
        "# Extract the best model identified by the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "print(\"Best model parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495de1c7-f56f-4c65-867a-6d0ef8a4cca9",
      "metadata": {
        "id": "495de1c7-f56f-4c65-867a-6d0ef8a4cca9"
      },
      "outputs": [],
      "source": [
        "# Example SMS messages for evaluation\n",
        "new_messages = [\n",
        "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
        "    \"Hey, are we still meeting up for lunch today?\",\n",
        "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
        "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
        "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a139ba88-399e-458a-ab09-56a5d6fb09fd",
      "metadata": {
        "id": "a139ba88-399e-458a-ab09-56a5d6fb09fd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "\n",
        "# Preprocess function that mirrors the training-time preprocessing\n",
        "def preprocess_message(message):\n",
        "    message = message.lower()\n",
        "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
        "    tokens = word_tokenize(message)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba960845-cd47-46ee-9110-b43a647f56ad",
      "metadata": {
        "id": "ba960845-cd47-46ee-9110-b43a647f56ad"
      },
      "outputs": [],
      "source": [
        "# Preprocess and vectorize messages\n",
        "processed_messages = [preprocess_message(msg) for msg in new_messages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717fb6c4-5321-4a26-bf88-8301b0e08dc6",
      "metadata": {
        "id": "717fb6c4-5321-4a26-bf88-8301b0e08dc6"
      },
      "outputs": [],
      "source": [
        "# Transform preprocessed messages into feature vectors\n",
        "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f2659c-b109-4c0c-90f4-c78de01f6c01",
      "metadata": {
        "id": "15f2659c-b109-4c0c-90f4-c78de01f6c01"
      },
      "outputs": [],
      "source": [
        "# Predict with the trained classifier\n",
        "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
        "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc5ea6c3-1908-4f3d-bfff-a8fcaf6c92aa",
      "metadata": {
        "id": "dc5ea6c3-1908-4f3d-bfff-a8fcaf6c92aa",
        "outputId": "3a8fa951-25a5-4f6a-dacd-93bd5e732818"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n",
            "Message: Hey, are we still meeting up for lunch today?\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
            "Prediction: Spam\n",
            "Spam Probability: 0.96\n",
            "Not-Spam Probability: 0.04\n",
            "--------------------------------------------------\n",
            "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
            "Prediction: Not-Spam\n",
            "Spam Probability: 0.00\n",
            "Not-Spam Probability: 1.00\n",
            "--------------------------------------------------\n",
            "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
            "Prediction: Spam\n",
            "Spam Probability: 1.00\n",
            "Not-Spam Probability: 0.00\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Display predictions and probabilities for each evaluated message\n",
        "for i, msg in enumerate(new_messages):\n",
        "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
        "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
        "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
        "\n",
        "    print(f\"Message: {msg}\")\n",
        "    print(f\"Prediction: {prediction}\")\n",
        "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
        "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41186ae1-aa1c-47a1-9a96-19e472ae7f65",
      "metadata": {
        "id": "41186ae1-aa1c-47a1-9a96-19e472ae7f65",
        "outputId": "d1225861-52b2-45e0-9c1e-1e13b85c7cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['spam_detection_model.joblib']"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file for future use\n",
        "model_filename = 'spam_detection_model.joblib'\n",
        "joblib.dump(best_model, model_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f8c453-cbfd-45b7-a525-f35082c35769",
      "metadata": {
        "id": "c6f8c453-cbfd-45b7-a525-f35082c35769",
        "outputId": "0540f723-b775-417b-9d24-699d355eb1d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to spam_detection_model.joblib\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model saved to {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fc4fe4-e094-4eeb-a4ab-97621911e4cb",
      "metadata": {
        "id": "06fc4fe4-e094-4eeb-a4ab-97621911e4cb"
      },
      "outputs": [],
      "source": [
        "# Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "\n",
        "# Preprocess new messages before prediction\n",
        "new_data_processed = [preprocess_message(msg) for msg in new_messages]\n",
        "\n",
        "# Make predictions on the preprocessed data\n",
        "predictions = loaded_model.predict(new_data_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27afc511-0407-4f51-98a5-8be3d6b4cd08",
      "metadata": {
        "id": "27afc511-0407-4f51-98a5-8be3d6b4cd08"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}